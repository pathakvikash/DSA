{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The correlation analysis is a statistical technique that measures the strength of the relationship between two variables. we are interested in finding the features that are most correlated with the target variable.\n",
    "Write a function that takes a dataset of numerical features and returns the features with the highest correlation with a target variable. Which feature(s) has the strongest correlation with the target variable, and how strong is the correlation?\n",
    "\n",
    "#\n",
    "Implement a function that takes a dataset of text features and returns a new dataset with the features transformed into a bag-of-words representation. Which words appear most frequently in the dataset, and what insights can be gained from this analysis?\n",
    "\n",
    "#\n",
    "Given a dataset of customer transactions, write a program that identifies the most common itemsets (i.e., sets of items that are frequently purchased together) and association rules (i.e., rules that describe the relationships between items in the dataset).\n",
    "\n",
    "#\n",
    "Implement a function that takes a dataset of time series data and returns the autocorrelation function (ACF) and partial autocorrelation function (PACF) for a specified variable. What insights can be gained from the ACF and PACF plots, and how can they be used to model the data?\n",
    "\n",
    "#\n",
    "Write a program that takes a dataset of images and performs object detection using convolutional neural networks (CNNs). Which objects are most frequently detected in the dataset, and how accurate are the detections? How can this analysis be used to improve the performance of the object detection model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RangeIndex(start=0, stop=4, step=1),\n",
       " Index(['page_transition', 'title', 'url', 'client_id', 'time_usec',\n",
       "        'favicon_url'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "data = df.head(4)\n",
    "data.describe\n",
    "data.values\n",
    "data.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m features, corr_values\n\u001b[1;32m     21\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m features, corr_values \u001b[39m=\u001b[39m select_features(data, \u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFeatures:\u001b[39m\u001b[39m'\u001b[39m, features)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCorrelation values:\u001b[39m\u001b[39m'\u001b[39m, corr_values)\n",
      "Cell \u001b[0;32mIn[54], line 16\u001b[0m, in \u001b[0;36mselect_features\u001b[0;34m(df, target)\u001b[0m\n\u001b[1;32m     12\u001b[0m corr \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcorr()\n\u001b[1;32m     13\u001b[0m \u001b[39m# corr_values = data.axes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# features = data.values\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Select the features with the highest correlation with the target variable\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m features \u001b[39m=\u001b[39m corr[target]\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m1\u001b[39m:\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     17\u001b[0m corr_values \u001b[39m=\u001b[39m corr[target][features]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m features, corr_values\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "\n",
    "# task 1 : Correlation analysis => compute the correlation matrix and select the features with the highest correlation with the target variable.\n",
    "\n",
    "def select_features(df, target):\n",
    "    # Remove non-numeric columns\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    data = df.head(4)\n",
    "    # Replace missing values with NaN\n",
    "    df.fillna(np.nan, inplace=True)\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "    corr_values = data.axes\n",
    "    features = data.values\n",
    "    # Select the features with the highest correlation with the target variable\n",
    "    features = corr[target].sort_values(ascending=False)[1:4].index.tolist()\n",
    "    corr_values = corr[target][features].tolist()\n",
    "\n",
    "    return features, corr_values\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "features, corr_values = select_features(data, 'title')\n",
    "print('Features:', features)\n",
    "print('Correlation values:', corr_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a spreadsheet of sales data and returns the top-performing products based on revenue. Which products generate the most revenue, and how much revenue do they generate? This could be useful for automating sales reporting and analysis.\n",
    "\n",
    "Implement a function that takes a folder of text documents and returns a new folder with the documents transformed into a standardized format with consistent formatting and spelling. This could be useful for automating the processing of large volumes of text documents.\n",
    "\n",
    "Given a dataset of customer reviews, write a program that identifies the most common topics and sentiments expressed in the reviews. Which topics are most frequently mentioned, and how do customers feel about them? This could be useful for automating sentiment analysis and customer feedback monitoring.\n",
    "\n",
    "Implement a function that takes a dataset of financial transactions and returns a report of the most common types of transactions and their frequency. Which types of transactions occur most frequently, and how much money is involved in each type of transaction? This could be useful for automating financial reporting and analysis.\n",
    "\n",
    "Write a program that takes a dataset of social media posts and performs sentiment analysis using machine learning algorithms. Which posts are most positive or negative, and how do they relate to specific products or services? This could be useful for automating social media monitoring and brand reputation management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a web scraping tool like Selenium or Puppeteer to extract browser history data from the user's browser. This could involve navigating to the appropriate page(s) in the browser and extracting the relevant HTML elements.\n",
    "\n",
    "Clean and preprocess the data by removing duplicates, filtering out irrelevant entries, and converting the data into a standardized format. This could involve using Python libraries like Pandas or NumPy to manipulate the data.\n",
    "\n",
    "Use natural language processing (NLP) techniques like named entity recognition (NER) and part-of-speech (POS) tagging to extract meaningful information from the browser history data. This could involve using Python libraries like NLTK or SpaCy to perform NLP tasks.\n",
    "\n",
    "Use a graph database like Neo4j to store the extracted data and create a knowledge graph. This could involve defining nodes and relationships between them based on the extracted information.\n",
    "\n",
    "Use graph visualization tools like Gephi or D3.js to explore and visualize the knowledge graph. This could involve creating interactive visualizations that allow users to explore the relationships between different nodes and entities in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python script that uses Selenium or Puppeteer to extract browser history data for a specified user. Which websites or webpages does the user visit most frequently, and how does this change over time?\n",
    "\n",
    "Implement a data cleaning and preprocessing pipeline using Pandas or NumPy to filter out duplicates and irrelevant entries from the browser history data. How much of the data is relevant, and how long does it take to process the data?\n",
    "\n",
    "Use NLP techniques like named entity recognition and part-of-speech tagging to extract meaningful information from the browser history data. Which entities or concepts are most frequently mentioned in the browser history, and how are they related to each other?\n",
    "\n",
    "Create a graph database using Neo4j to store the extracted data and create a knowledge graph. What nodes and relationships should be defined based on the extracted information, and how can the graph be optimized for querying and visualization?\n",
    "\n",
    "Use graph visualization tools like Gephi or D3.js to explore and visualize the knowledge graph. How can the graph be visualized to highlight patterns or anomalies in the data, and how can users interact with the graph to explore the relationships between different nodes and entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a headless browser like PhantomJS or Chrome Headless instead of a full-fledged browser like Chrome or Firefox. This can reduce the resource requirements and improve the performance of the scraping process.\n",
    "\n",
    "Implement error handling and retry mechanisms to handle network errors, server timeouts, or other issues that may arise during the scraping process. This can improve the reliability and robustness of the scraping process.\n",
    "\n",
    "Use a data pipeline framework like Apache Beam or Apache NiFi to automate the data cleaning and preprocessing steps. This can reduce the amount of manual work required to clean and preprocess the data, and can help ensure that the data is consistent and standardized.\n",
    "\n",
    "Use a graph query language like Cypher to query the graph database and extract meaningful insights from the data. This can help automate the analysis process and make it easier to extract insights from the data.\n",
    "\n",
    "Implement a web interface or API that allows users to interact with the knowledge graph and visualize the data in real-time. This can make it easier for users to explore the data and gain insights from it, and can help make the tool more user-friendly and accessible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
